{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3200, 1, 64, 64])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.load(\"x_train.npz\")[\"arr_0\"]\n",
    "y_train = np.load(\"y_train.npz\")[\"arr_0\"]\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, 64, 64)\n",
    "X_train = torch.Tensor(X_train)\n",
    "y_train = torch.Tensor(y_train)\n",
    "\n",
    "train_ds = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_ds)\n",
    "\n",
    "X_test = np.load(\"x_test.npz\")[\"arr_0\"]\n",
    "y_test = np.load(\"y_test.npz\")[\"arr_0\"]\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, 64, 64)\n",
    "X_test = torch.Tensor(X_test)\n",
    "y_test = torch.Tensor(y_test)\n",
    "\n",
    "test_ds = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_ds)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M7_1(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes=160):\n",
    "        super(M7_1, self).__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(self.in_channels, 64, kernel_size=3, padding=1), # (64, 64, 64)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), # (32, 32, 64)\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1), # (32, 32, 128)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), # (16, 16, 128)\n",
    "\n",
    "            nn.Conv2d(128, 512, kernel_size=3, padding=1), # (16, 16, 512)\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1), # (16, 16, 512)\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2) # (8, 8, 512)\n",
    "        )\n",
    "\n",
    "        self.linear_layers = nn.Sequential(\n",
    "            nn.Linear(in_features=8*8*512, out_features=4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Linear(in_features=4096, out_features=4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Linear(in_features=4096, out_features=self.num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model: torch.nn.Module, \n",
    "               dataloader: torch.utils.data.DataLoader, \n",
    "               loss_fn: torch.nn.Module, \n",
    "               optimizer: torch.optim.Optimizer):\n",
    "    # Put model in train mode\n",
    "    model.train()\n",
    "    \n",
    "    # Setup train loss and train accuracy values\n",
    "    train_loss, train_acc = 0, 0\n",
    "    \n",
    "    # Loop through data loader data batches\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Send data to target device\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # 1. Forward pass\n",
    "        y_pred = model(X)\n",
    "\n",
    "        # 2. Calculate  and accumulate loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item() \n",
    "\n",
    "        # 3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate and accumulate accuracy metric across all batches\n",
    "        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "        train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
    "\n",
    "    # Adjust metrics to get average loss and accuracy per batch \n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_acc = train_acc / len(dataloader)\n",
    "    return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(model: torch.nn.Module, \n",
    "              dataloader: torch.utils.data.DataLoader, \n",
    "              loss_fn: torch.nn.Module):\n",
    "    # Put model in eval mode\n",
    "    model.eval() \n",
    "    \n",
    "    # Setup test loss and test accuracy values\n",
    "    test_loss, test_acc = 0, 0\n",
    "    \n",
    "    # Turn on inference context manager\n",
    "    with torch.inference_mode():\n",
    "        # Loop through DataLoader batches\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            # Send data to target device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "    \n",
    "            # 1. Forward pass\n",
    "            test_pred_logits = model(X)\n",
    "\n",
    "            # 2. Calculate and accumulate loss\n",
    "            loss = loss_fn(test_pred_logits, y)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            # Calculate and accumulate accuracy\n",
    "            test_pred_labels = test_pred_logits.argmax(dim=1)\n",
    "            test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
    "            \n",
    "    # Adjust metrics to get average loss and accuracy per batch \n",
    "    test_loss = test_loss / len(dataloader)\n",
    "    test_acc = test_acc / len(dataloader)\n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "# 1. Take in various parameters required for training and test steps\n",
    "def train(model: torch.nn.Module, \n",
    "          train_dataloader: torch.utils.data.DataLoader, \n",
    "          test_dataloader: torch.utils.data.DataLoader, \n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          loss_fn: torch.nn.Module = nn.CrossEntropyLoss(),\n",
    "          epochs: int = 5):\n",
    "    \n",
    "    # 2. Create empty results dictionary\n",
    "    results = {\"train_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"test_loss\": [],\n",
    "        \"test_acc\": []\n",
    "    }\n",
    "    \n",
    "    # 3. Loop through training and testing steps for a number of epochs\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc = train_step(model=model,\n",
    "                                           dataloader=train_dataloader,\n",
    "                                           loss_fn=loss_fn,\n",
    "                                           optimizer=optimizer)\n",
    "        test_loss, test_acc = test_step(model=model,\n",
    "            dataloader=test_dataloader,\n",
    "            loss_fn=loss_fn)\n",
    "        \n",
    "        # 4. Print out what's happening\n",
    "        print(\n",
    "            f\"Epoch: {epoch+1} | \"\n",
    "            f\"train_loss: {train_loss:.4f} | \"\n",
    "            f\"train_acc: {train_acc:.4f} | \"\n",
    "            f\"test_loss: {test_loss:.4f} | \"\n",
    "            f\"test_acc: {test_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "        # 5. Update results dictionary\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "    # 6. Return the filled results at the end of the epochs\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [45:13<3:00:55, 2713.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 6.3346 | train_acc: 0.9994 | test_loss: 4.2585 | test_acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [1:25:54<2:07:38, 2552.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | train_loss: 4.3247 | train_acc: 0.6075 | test_loss: 3.6443 | test_acc: 0.0100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [2:04:44<1:21:42, 2451.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | train_loss: 3.4549 | train_acc: 1.8862 | test_loss: 3.0011 | test_acc: 0.0175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [2:42:43<39:43, 2383.42s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | train_loss: 2.5785 | train_acc: 2.0106 | test_loss: 2.2882 | test_acc: 0.0825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [3:20:18<00:00, 2403.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | train_loss: 1.8837 | train_acc: 1.5219 | test_loss: 1.8537 | test_acc: 0.2600\n",
      "Total training time: 12018.135 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds\n",
    "torch.manual_seed(42) \n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "# Set number of epochs\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "# Recreate an instance of TinyVGG\n",
    "model_0 = M7_1(in_channels=1, num_classes=50).to(device)\n",
    "\n",
    "# Setup loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model_0.parameters(), lr=0.001)\n",
    "\n",
    "# Start the timer\n",
    "from timeit import default_timer as timer \n",
    "start_time = timer()\n",
    "\n",
    "# Train model_0 \n",
    "model_0_results = train(model=model_0, \n",
    "                        train_dataloader=train_loader,\n",
    "                        test_dataloader=test_loader,\n",
    "                        optimizer=optimizer,\n",
    "                        loss_fn=loss_fn, \n",
    "                        epochs=NUM_EPOCHS)\n",
    "\n",
    "# End the timer and print out how long it took\n",
    "end_time = timer()\n",
    "print(f\"Total training time: {end_time-start_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"state_dict_model2.pt\"\n",
    "torch.save(model_0.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class M7_1(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes=160):\n",
    "        super(M7_1, self).__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(self.in_channels, 64, kernel_size=3, padding=1), # (64, 64, 64)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), # (32, 32, 64)\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1), # (32, 32, 128)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), # (16, 16, 128)\n",
    "\n",
    "            nn.Conv2d(128, 512, kernel_size=3, padding=1), # (16, 16, 512)\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1), # (16, 16, 512)\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2) # (8, 8, 512)\n",
    "        )\n",
    "\n",
    "        self.linear_layers = nn.Sequential(\n",
    "            nn.Linear(in_features=8*8*512, out_features=4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Linear(in_features=4096, out_features=4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Linear(in_features=4096, out_features=self.num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -11.7692,  -12.7077,  -12.5295,  -12.5333,  -11.8978,   -9.9113,\n",
       "           -9.5400,  -13.3652,  -11.4358,  -16.1090,  -13.9973,   -6.8059,\n",
       "           -8.6441,   -9.8452,  -11.1677,  -12.6952,  -10.7024,  -14.1324,\n",
       "          -11.5763,  -12.0092,  -10.5967,  -11.9948,   -8.0938,  -13.7596,\n",
       "          -12.4669,  -12.8741,   -9.4223,  -13.7820,  -12.3763,  -15.5752,\n",
       "          -12.5236,  -14.8909,  -12.6788,  -14.3277,  -11.0124,   -9.5081,\n",
       "          -10.6029,  -10.7399,   -9.2584,  -12.7412,   -5.2719,   -3.4851,\n",
       "           -6.2773,  -11.5325,   -9.7293,  -10.0259,  -11.6191,   -6.6694,\n",
       "           -8.8812,  -16.6115],\n",
       "        [ -66.5205,  -57.6763,  -61.5176,  -69.2670,  -78.1169,  -59.4901,\n",
       "          -88.6364,  -82.8598,  -84.1030,  -59.6555,  -57.1896,  -64.5473,\n",
       "          -76.0834,  -71.0329,  -85.4221,  -73.1917,  -87.6762,  -73.4854,\n",
       "          -81.1785,  -71.2587,  -79.0673,  -91.7727, -102.6079,  -66.6925,\n",
       "          -78.4900,  -79.5802,  -89.5471,  -61.3253,  -75.4831,  -67.5942,\n",
       "          -74.7262,  -69.7796,  -77.7718,  -70.4847,  -79.0193,  -75.9394,\n",
       "          -80.5677,  -66.4053,  -79.3256,  -15.2433,  -81.8883,  -96.8818,\n",
       "         -105.4940,  -75.0051,  -82.7751,  -79.5512,  -80.1956,  -95.6795,\n",
       "         -103.6890,  -66.8688],\n",
       "        [ -28.2326,  -21.4157,  -25.9857,  -22.4367,  -32.0808,  -24.7383,\n",
       "          -27.5744,  -16.8951,  -26.6794,  -19.4765,  -21.9445,  -19.3530,\n",
       "          -26.7557,  -18.5668,  -24.9857,  -17.9373,  -27.3482,  -17.1654,\n",
       "          -24.2643,  -22.4152,  -28.0364,  -15.9225,  -27.9280,  -21.1202,\n",
       "          -24.7392,  -22.3464,  -25.0198,  -24.3696,  -27.6501,  -26.4520,\n",
       "          -27.8924,  -22.7564,  -22.7946,  -21.0153,  -23.1180,  -20.2212,\n",
       "          -14.1025,  -26.4648,  -22.1502,  -25.0864,  -14.3596,  -28.4224,\n",
       "          -29.6291,  -17.8528,  -22.9140,  -21.3497,  -21.5683,  -24.8103,\n",
       "          -25.2481,  -24.8905],\n",
       "        [ -14.3618,  -18.7667,  -20.0489,  -16.5819,  -15.9562,  -13.6865,\n",
       "          -14.6637,  -15.6315,  -15.7018,  -21.1467,  -19.5814,  -14.1027,\n",
       "          -16.9585,  -15.9151,  -17.0082,  -17.3566,  -15.0872,  -19.9272,\n",
       "          -18.1502,  -16.3628,  -17.0921,  -14.7138,  -14.0383,  -15.9364,\n",
       "          -17.5881,  -14.7633,  -14.7732,  -18.5573,  -18.4985,  -23.7546,\n",
       "          -21.9299,  -20.7055,  -17.4659,  -19.8424,  -18.0372,  -13.1763,\n",
       "          -14.6188,  -13.0560,  -12.0565,  -16.8283,  -10.0070,  -13.5747,\n",
       "          -12.5627,  -15.1980,  -15.1419,  -13.9427,   -8.7603,   -5.7563,\n",
       "           -0.9471,  -20.9230],\n",
       "        [ -55.3054,  -49.4876,  -65.7812,  -50.3865,  -66.7936,  -55.7769,\n",
       "          -68.6650,  -67.2151,  -84.7556,  -61.6659,  -70.8982,  -51.7560,\n",
       "          -64.8441,  -53.5198,  -71.9002,  -69.4426,  -82.9696,  -55.7547,\n",
       "          -71.3682,  -56.3468,  -68.8676,  -67.9056,  -85.4504,  -49.3506,\n",
       "          -78.4221,  -61.4015,  -74.9084,  -72.7694,  -73.7442,  -81.5224,\n",
       "          -81.4781,  -62.4080,  -63.9438,  -52.1507,  -63.7904,  -71.1317,\n",
       "          -55.1346,  -59.2491,  -54.8421,  -72.8853,  -61.6025,  -78.4606,\n",
       "          -93.8447,  -51.2178,  -65.9371,  -61.7926,   -8.8241,  -46.0844,\n",
       "          -46.4576,  -54.6379],\n",
       "        [ -23.8169,  -27.3493,  -30.3885,  -28.8486,  -24.7919,  -21.5630,\n",
       "          -21.2596,  -29.2126,  -19.3673,  -29.6883,  -25.8615,  -24.6931,\n",
       "          -24.0865,  -29.9333,  -21.4992,  -26.8225,  -18.4471,  -29.6733,\n",
       "          -26.7764,  -28.5022,  -21.8530,  -20.3421,   -3.5600,  -27.9116,\n",
       "          -17.5013,  -27.8158,  -15.2242,  -30.4851,  -24.2443,  -34.9324,\n",
       "          -24.7419,  -28.9732,  -22.1344,  -27.3605,  -21.1876,  -25.4618,\n",
       "          -26.8703,  -28.0335,  -25.5580,  -25.8891,  -25.5560,  -20.1994,\n",
       "          -26.8309,  -25.8477,  -20.0092,  -24.6855,  -31.2200,  -24.9617,\n",
       "          -29.6767,  -32.7596]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "X_test = np.load(\"x_test.npz\")[\"arr_0\"]\n",
    "X_test = X_test[:6]\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, 64, 64)\n",
    "X_test = torch.Tensor(X_test)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = M7_1(in_channels=1, num_classes=50)\n",
    "model.load_state_dict(torch.load(\"state_dict_model_20_epoch.pt\"))\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "X_test = X_test.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred = model(X_test)\n",
    "\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "0.8225\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "\n",
    "X_test = np.load(\"x_test.npz\")[\"arr_0\"]\n",
    "#X_test = X_test[:60]\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, 64, 64)\n",
    "X_test = torch.Tensor(X_test)\n",
    "y_test = np.load(\"y_test.npz\")[\"arr_0\"]\n",
    "#y_test = y_test[:60]\n",
    "y_test = torch.Tensor(y_test)\n",
    "\n",
    "val_ds = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "val_loader = torch.utils.data.DataLoader(val_ds)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = M7_1(in_channels=1, num_classes=50)\n",
    "model.load_state_dict(torch.load(\"state_dict_model_20_epoch.pt\"))\n",
    "# model = torch.load(\"state_dict_model_20_epoch.pt\")\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "s = 0\n",
    "print(len(val_ds.tensors))\n",
    "for i, (images, labels) in enumerate(val_loader):\n",
    "    images = images.to(device)\n",
    "    preds = model(images)\n",
    "    # _, preds = preds.detach().cpu().numpy()\n",
    "    # labels = labels.detach().numpy()\n",
    "    # print(preds)\n",
    "    # print(labels)\n",
    "    # print(np.argmax(preds.detach().numpy()), end=\"\\t\")\n",
    "    # print(np.argmax(labels.detach().numpy()))\n",
    "    s += (np.argmax(preds.detach().numpy()) == np.argmax(labels.detach().numpy()))\n",
    "print(s/X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "arr[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(preds.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = np.load(\"y_test.npz\")[\"arr_0\"]\n",
    "np.argmax(y_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCABAAEABAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+iiiiiiiiiiug0TwN4p8R+Q2k6DfXEM+7y7jyikLbc5/ethByCOT14610Fz8FPH1rpzXj6Hv27y8MVxG8iqoBztDHdnJAC5bKnjkZ8/oooor1j4SfCT/hM/8Aida158GjQyqI4wmPtuN24BsgqoIUEgc5YAggkfTc81joWjSzuI7XT7C3LsI4/liiRc8Ko6BR0A7cV4Bpv7SepDWWbVNDtG0t3IVLVmE8SlhyWYlXIXPGEyccrXX+M/COi/FrwGnibw3ZRjWJU862mZRC8xUhHilOMMQEKgk4BUYYKST8uUUVc0nTZtZ1mx0u3aNZ724jt42kJChnYKCcAnGT6Gvp/wCJF+3w8+GWneHfC5kjv7p49OshAyi4Ixl5FVVyzseCVAO6UHIOMx/GG5/4Q/4Mw6NYNOY5fI0pJjLtdY1UklsD5tyxFSOAd57cH5Yr6f8A2cf+Seah/wBhWT/0VFXzp4lmsbnxVq8+liMafJezPaiOPYoiLkphcDaNuOMDFZdFbHhO+t9M8ZaHf3knl2trqFvNM+0naiyKWOBycAHpX1n4v8FTeKPGPhDVDPHHZ6LcS3E67yJHb920YX5SCN0Y3ZI4PHNeMftFa+uoeMbHRYmjZNLty0mFYMssuGKkngjYsRGP7x57Dxuvq/8AcfCT4G/8+mpi0/6ZNI19Kv8A3y+1j/tfJH/FivlCiiiu40b4u+NtB0dNLstYzaxRCKATQRyNAAwI2swycAFQGyApwAMDHFzzzXVxLcXEsk08rl5JJGLM7E5JJPJJPOa7j4SeC5vGPjW2DJH/AGfp7pdXhmhMiOqsMREdCXwRg9gx5xg9J8d/HuneKNR0/SdFvftVjY+Y88sTt5ckxO0DBADbQpIcEgiQ4758fooooorQstc1TTdOvbCxv57a1vtoukhfb5wUMArEclcO2V6HPIOBjPoooooooooor//Z",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAAAAACPAi4CAAABMElEQVR4Ae2RSxLDMAhD007vf+UWO4FgIXncTVbOxh+kh0yOY397AnsCj03gO+v0mhWPI7xSJwvJ3HsoobpHv0G4lN+aPMKnNzLxO9XzlvmXoWP/1jZwNUO9yXrbX4ILUeXqCWYdvtdpjShRZICsulsKAgFkfzSSGwI4tXdr99abVqmAM8ClribMVwG9YTV6DlwLwDv4igY8F0AXeABf0ZXOCFhtHAgEREFtMBQFmGg5CQV4d+zm93mdArJQhfoD0HEllAD0EYxiMRUBGMPPTh9WFM1MOoZqXkyQFGlrQgVFQIPqb2R2XQGEJjaa1yoF4M8EP/ktJ5gM0axfsE9C1ARNvO4nT6Dd5AsWAeofWjMyAxpBPovPABgtgBrLEmDih1biOBmBcOzrPYE9gecn8ANNXiBPEuN5TwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=64x64>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "X_test = np.load(\"x_test.npz\")[\"arr_0\"]\n",
    "Image.fromarray(X_test[1].astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.array([[1, 2, 3], [4, 5, 6]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (4) must match the size of tensor b (2) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/hermann/programming/kanji-recognizer/02_cnn.ipynb Cell 18\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hermann/programming/kanji-recognizer/02_cnn.ipynb#X23sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m a \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor([\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hermann/programming/kanji-recognizer/02_cnn.ipynb#X23sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m b \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor([\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m])\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/hermann/programming/kanji-recognizer/02_cnn.ipynb#X23sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m a \u001b[39m==\u001b[39;49m b\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (4) must match the size of tensor b (2) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.Tensor([0, 0, 1, 0])\n",
    "b = torch.Tensor([1, 0])\n",
    "a == b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
